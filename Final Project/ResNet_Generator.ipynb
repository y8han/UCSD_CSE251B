{"metadata":{"language_info":{"name":"python","version":"3.7.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n#nc = 3 # Number of channels in the training images. For color images this is 3\n#nz = 100 # Size of z latent vector (i.e. size of generator input)\n#ngf = 64 # Size of feature maps in generator\n#ngpu = 1 # Number of GPUs available. Use 0 for CPU mode.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# custom weights initialization called on netG and netD\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        m.weight.data.normal_(0.0, 0.02)\n    elif classname.find('BatchNorm2d') != -1 or  classname.find('InstanceNorm2d') != -1 or classname.find('LayerNorm') != -1\n        m.weight.data.normal_(1.0, 0.02)\n        m.bias.data.fill_(0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def define_G(input_nc, output_nc, ngf, which_model_netG, norm, use_dropout=False):\n    netG = None\n    use_gpu = len(gpu_ids) > 0\n    if norm == 'batch':\n        norm_layer = nn.BatchNorm2d\n    elif norm == 'layer':\n        norm_layer = nn.LayerNorm\n    elif norm == 'instance':\n        norm_layer = nn.InstanceNorm2d\n    else:\n        print('normalization layer [%s] is not found' % norm)\n    if use_gpu:\n        assert(torch.cuda.is_available())\n\n    if which_model_netG == 'resnet_9blocks':\n        netG = ResnetGenerator(input_nc, output_nc, ngf, norm_layer, use_dropout=use_dropout, n_blocks=9)\n    elif which_model_netG == 'resnet_6blocks':\n        netG = ResnetGenerator(input_nc, output_nc, ngf, norm_layer, use_dropout=use_dropout, n_blocks=6)\n#    elif which_model_netG == 'unet_128':\n#        netG = UnetGenerator(input_nc, output_nc, 7, ngf, norm_layer, use_dropout=use_dropout)\n#    elif which_model_netG == 'unet_256':\n#        netG = UnetGenerator(input_nc, output_nc, 8, ngf, norm_layer, use_dropout=use_dropout)\n    else:\n        print('Generator model name [%s] is not recognized' % which_model_netG)\n    netG.apply(weights_init)\n    return netG","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defines the ResNet generator\nclass ResnetGenerator(nn.Module):\n    def __init__(self, input_nc, output_nc, ngf=64, norm_layer=nn.InstanceNorm2d, use_dropout=False, n_blocks=6):\n        assert(n_blocks >= 0)\n        super(ResnetGenerator, self).__init__()\n        self.input_nc = input_nc\n        self.output_nc = output_nc\n        self.ngf = ngf\n\n        model = [nn.ReflectionPad2d(3),\n                 nn.Conv2d(input_nc, ngf, kernel_size=7, padding=3),\n                 norm_layer(ngf),\n                 nn.ReLU(True)]\n\n        n_downsampling = 2\n        for i in range(n_downsampling):\n            mult = 2**i\n            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3,\n                                stride=2, padding=1),\n                      norm_layer(ngf * mult * 2),\n                      nn.ReLU(True)]\n\n        mult = 2**n_downsampling\n        for i in range(n_blocks):\n            model += [ResnetBlock(ngf * mult, 'zero', norm_layer=norm_layer, use_dropout=use_dropout)]\n\n        for i in range(n_downsampling):\n            mult = 2**(n_downsampling - i)\n            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2),\n                                         kernel_size=3, stride=2,\n                                         padding=1, output_padding=1),\n                      norm_layer(int(ngf * mult / 2)),\n                      nn.ReLU(True)]\n        \n        model += [nn.ReflectionPad2d(3)]\n        model += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=3)]\n        model += [nn.Tanh()]\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, input):\n         return self.model(input)\n\n\n# Define a resnet block\nclass ResnetBlock(nn.Module):\n    def __init__(self, dim, padding_type, norm_layer, use_dropout):\n        super(ResnetBlock, self).__init__()\n        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, use_dropout)\n\n    def build_conv_block(self, dim, padding_type, norm_layer, use_dropout):\n        conv_block = []\n        conv_block += [nn.ReflectionPad2d(1)]\n        \n        p = 0\n        # TODO: support padding types\n        assert(padding_type == 'zero')\n        p = 1\n\n        # TODO: InstanceNorm\n        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p),\n                       norm_layer(dim),\n                       nn.ReLU(True)]\n        if use_dropout:\n            conv_block += [nn.Dropout(0.5)]\n        conv_block += [nn.ReflectionPad2d(1)]\n        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p),\n                       norm_layer(dim)]\n\n        return nn.Sequential(*conv_block)\n\n    def forward(self, x):\n        out = x + self.conv_block(x)\n        return out","metadata":{},"execution_count":null,"outputs":[]}]}
